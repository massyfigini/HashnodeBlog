---
title: "Copy data from a Blob Storage to a Sharepoint folder with Azure Data Factory through Azure Logic App"
datePublished: Fri Jun 17 2022 22:00:00 GMT+0000 (Coordinated Universal Time)
cuid: clx96u09300020amc0qhy6qu2
slug: copy-data-from-a-blob-storage-to-a-sharepoint-folder-with-azure-data-factory-through-azure-logic-app
tags: azure, sharepoint, azure-logic-apps, adf, azure-data-factory

---

### 1\. Create the Logic App

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660229263301/RO1iOMwTw.png align="left")

Choose the "Consuption" type.

### 2\. Design the Logic App

Start with the common trigger "When a HTTP request is received"

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660229545930/HQYtflpXD.png align="left")

In the JSON body add all the parameters you need to use:

```sql
{
    "properties": {
        "Blob": {
            "type": "string"
        },
        "DataFactoryName": {
            "type": "string"
        },
        "Filename": {
            "type": "string"
        },
        "Folder": {
            "type": "string"
        }
    },
    "type": "object"
}
```

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660229596609/NAglEBJ7I.png align="left")

An HTTP POST URL will appear.

Add a new action step: "Azure Blob Storage - Get blob content (V2)".  
Choose how you want to connect to the blob storage. I used the Access Key method: in this case you have to choose a name for the connection, then add the storage account name and its access key and click "Create".

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660230325805/7NGQckwKT.png align="left")

After that in the new box that appear, choose "Use connection settings" in the storage account name and specify the file you want to upload to Sharepoint.

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660230430917/ipHqUqgyP.png align="left")

The last step is the Sharepoint action "Create file". Like before, choose a connection method. Then add in the appropriate space the Sharepoint site address, and the parameters defined in the first step in the "Folder Path" and "File Name" spaces using the "Add dynamic content" link.  
In the "File content" space, choose again the "Add dynamic content" link and the File Content of the Get blob content (V2) defined earlier.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718036777998/36fcd172-1a7e-45c0-81a9-ddfc4f7ed589.png align="center")

The Logic App is now ready.

### 3\. Add the new step to your ADF pipeline

In Azure Data Factory Studio, add three string parameters to your pipeline: Filename, Folder and Blob. Assign a name to the Sharepoint Filename (with the correct extension) to the first, the Sharepoint folder name to the second and the blob address and filename from your Storage Account to the third.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718036642861/b11e368a-ad1c-4f07-80b9-d4c8931ee1fb.png align="center")

In your Azure Data Factory pipeline, add a new Web activity.  
In its Settings tab, add the URL you have copied from the Design Logic App first step in the URL space, choose POST as method and choose "Add dynamic content" under the body space after clicked on it.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718036556666/245025e5-34ed-4be7-841c-cc3b59576209.png align="center")

Add the parameters in this format:

```sql
{
"DataFactoryName": "@{pipeline().DataFactory}",
"Filename": "@{pipeline().parameters.Filename}",
"Folder": "@{pipeline().parameters.Folder}",
"Blob": "@{pipeline().parameters.Blob}"
}
```

![image.png](https://cdn.hashnode.com/res/hashnode/image/upload/v1660233873583/ISmBIEnsQ.png align="left")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1718036698789/e86dd52a-e82b-413f-ac58-b28eaa07546e.png align="center")

Choose your Integration Runtime and publish all.

You now can trigger the pipeline.